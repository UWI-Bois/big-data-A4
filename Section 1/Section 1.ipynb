{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID : 816000325\n",
    "## Name: Ajay Sieunarine\n",
    "## Email: ajay.sieunarine@my.uwi.edu\n",
    "## Repo: https://github.com/jefroy/big-data-A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\idisc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\idisc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# math and graph stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# text analysis stuff\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "# ML stuff\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "# utility stuff\n",
    "from sklearn.utils import shuffle\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# more graph stuff\n",
    "%matplotlib inline\n",
    "plt.style.use('dark_background') # turn off for light theme\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "plt.rcParams[\"xtick.labelsize\"] = 7\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "sep = '============================================================================================'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Up (20 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[2 0 2]\n",
      " [2 2 0]\n",
      " [2 4 0]\n",
      " [0 4 2]\n",
      " [1 4 1]]\n"
     ]
    }
   ],
   "source": [
    "# documents/sentences\n",
    "corpus = [ \n",
    "    'Apple Orange Orange Apple',\n",
    "    'Apple Banana Apple Banana',\n",
    "    'Banana Apple Banana Banana Banana Apple',\n",
    "    'Banana Orange Banana Banana Orange Banana',\n",
    "    'Banana Apple Banana Banana Orange Banana'\n",
    "]\n",
    "\n",
    "def corp2vec(corpus):\n",
    "    '''\n",
    "    This function converts a document to a vector showing its term frequencies.\n",
    "    Each column represents a term in the corpus. (in this case, there are 3 terms).\n",
    "    Uses CountVectorizer() as a model.\n",
    "    '''\n",
    "    tf_vectorizer = CountVectorizer() # model\n",
    "    tf = tf_vectorizer.fit_transform(corpus) # get term frequencies\n",
    "    return tf.A # return the array representation\n",
    "\n",
    "print(type(corp2vec(corpus)))\n",
    "print(corp2vec(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Data Organization (20 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>playlist</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>title</th>\n",
       "      <th>view_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>like_count</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>subtitles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2WTNSujhjk</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20160220</td>\n",
       "      <td>Live Stream: Donald Trump Victory Rally in Spa...</td>\n",
       "      <td>4057.0</td>\n",
       "      <td>4.259259</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>presidents of the United States mr. go   tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-64nfy6i58w</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20161107</td>\n",
       "      <td>LAST RALLY: Donald Trump FINAL CAMPAIGN Rally ...</td>\n",
       "      <td>47276.0</td>\n",
       "      <td>4.358025</td>\n",
       "      <td>952.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>it's now officially Tuesday November a   di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7Sp31hTxkU</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20160423</td>\n",
       "      <td>FULL SPEECH: Donald Trump Rally in Bridgeport,...</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>220.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>you   [Music]   [Music]   [Music]   you   I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-byuyavcNI4</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20160617</td>\n",
       "      <td>Full Speech: Donald Trump Rally in Houston, Te...</td>\n",
       "      <td>15138.0</td>\n",
       "      <td>4.582491</td>\n",
       "      <td>266.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>we welcome stars and president   [Music]   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09BXh-AA72M</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20161105</td>\n",
       "      <td>Full Speech: Donald Trump Rally in Denver, Col...</td>\n",
       "      <td>8720.0</td>\n",
       "      <td>4.924731</td>\n",
       "      <td>365.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>you   thank you   [Music]   great people Gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                        playlist  upload_date  \\\n",
       "0  -2WTNSujhjk  Donald Trump Speeches & Events     20160220   \n",
       "1  -64nfy6i58w  Donald Trump Speeches & Events     20161107   \n",
       "2  -7Sp31hTxkU  Donald Trump Speeches & Events     20160423   \n",
       "3  -byuyavcNI4  Donald Trump Speeches & Events     20160617   \n",
       "4  09BXh-AA72M  Donald Trump Speeches & Events     20161105   \n",
       "\n",
       "                                               title  view_count  \\\n",
       "0  Live Stream: Donald Trump Victory Rally in Spa...      4057.0   \n",
       "1  LAST RALLY: Donald Trump FINAL CAMPAIGN Rally ...     47276.0   \n",
       "2  FULL SPEECH: Donald Trump Rally in Bridgeport,...     19966.0   \n",
       "3  Full Speech: Donald Trump Rally in Houston, Te...     15138.0   \n",
       "4  Full Speech: Donald Trump Rally in Denver, Col...      8720.0   \n",
       "\n",
       "   average_rating  like_count  dislike_count  \\\n",
       "0        4.259259        44.0           10.0   \n",
       "1        4.358025       952.0          182.0   \n",
       "2        4.666667       220.0           20.0   \n",
       "3        4.582491       266.0           31.0   \n",
       "4        4.924731       365.0            7.0   \n",
       "\n",
       "                                           subtitles  \n",
       "0     presidents of the United States mr. go   tr...  \n",
       "1     it's now officially Tuesday November a   di...  \n",
       "2     you   [Music]   [Music]   [Music]   you   I...  \n",
       "3     we welcome stars and president   [Music]   ...  \n",
       "4     you   thank you   [Music]   great people Gr...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('MrTrumpSpeeches.csv', sep='~', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# remove this line please\n",
    "# df = df[:10]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 object\n",
       "playlist           object\n",
       "upload_date         int64\n",
       "title              object\n",
       "view_count        float64\n",
       "average_rating    float64\n",
       "like_count        float64\n",
       "dislike_count     float64\n",
       "subtitles          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: \n",
    "Create a new column in the dataframe called 'sentiment'. Using appropriate existing columns, populate the new column with 0's and 1's where 0 refers to a negative sentiment and 1 refers to a positive sentiment. \n",
    "\n",
    "- using the `average_rating` column, find the average score\n",
    "    * ratings above or equal to this average are denoted by sentiment = 1\n",
    "    * ratings under this average are denoted by sentiment = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "1.34782612324\n",
      "3.17391306162\n"
     ]
    }
   ],
   "source": [
    "# find the avg/mid score possible:\n",
    "max_score = max(df['average_rating'])\n",
    "min_score = min(df['average_rating'])\n",
    "avg_score = (max_score + min_score) / 2\n",
    "print(max_score)\n",
    "print(min_score)\n",
    "print(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "============================================================================================\n",
      "count    836.000000\n",
      "mean       0.903110\n",
      "std        0.295985\n",
      "min        0.000000\n",
      "25%        1.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: sentiment, dtype: float64\n",
      "============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>playlist</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>title</th>\n",
       "      <th>view_count</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>like_count</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>subtitles</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2WTNSujhjk</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20160220</td>\n",
       "      <td>Live Stream: Donald Trump Victory Rally in Spa...</td>\n",
       "      <td>4057.0</td>\n",
       "      <td>4.259259</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>presidents of the United States mr. go   tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-64nfy6i58w</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20161107</td>\n",
       "      <td>LAST RALLY: Donald Trump FINAL CAMPAIGN Rally ...</td>\n",
       "      <td>47276.0</td>\n",
       "      <td>4.358025</td>\n",
       "      <td>952.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>it's now officially Tuesday November a   di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7Sp31hTxkU</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20160423</td>\n",
       "      <td>FULL SPEECH: Donald Trump Rally in Bridgeport,...</td>\n",
       "      <td>19966.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>220.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>you   [Music]   [Music]   [Music]   you   I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-byuyavcNI4</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20160617</td>\n",
       "      <td>Full Speech: Donald Trump Rally in Houston, Te...</td>\n",
       "      <td>15138.0</td>\n",
       "      <td>4.582491</td>\n",
       "      <td>266.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>we welcome stars and president   [Music]   ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09BXh-AA72M</td>\n",
       "      <td>Donald Trump Speeches &amp; Events</td>\n",
       "      <td>20161105</td>\n",
       "      <td>Full Speech: Donald Trump Rally in Denver, Col...</td>\n",
       "      <td>8720.0</td>\n",
       "      <td>4.924731</td>\n",
       "      <td>365.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>you   thank you   [Music]   great people Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                        playlist  upload_date  \\\n",
       "0  -2WTNSujhjk  Donald Trump Speeches & Events     20160220   \n",
       "1  -64nfy6i58w  Donald Trump Speeches & Events     20161107   \n",
       "2  -7Sp31hTxkU  Donald Trump Speeches & Events     20160423   \n",
       "3  -byuyavcNI4  Donald Trump Speeches & Events     20160617   \n",
       "4  09BXh-AA72M  Donald Trump Speeches & Events     20161105   \n",
       "\n",
       "                                               title  view_count  \\\n",
       "0  Live Stream: Donald Trump Victory Rally in Spa...      4057.0   \n",
       "1  LAST RALLY: Donald Trump FINAL CAMPAIGN Rally ...     47276.0   \n",
       "2  FULL SPEECH: Donald Trump Rally in Bridgeport,...     19966.0   \n",
       "3  Full Speech: Donald Trump Rally in Houston, Te...     15138.0   \n",
       "4  Full Speech: Donald Trump Rally in Denver, Col...      8720.0   \n",
       "\n",
       "   average_rating  like_count  dislike_count  \\\n",
       "0        4.259259        44.0           10.0   \n",
       "1        4.358025       952.0          182.0   \n",
       "2        4.666667       220.0           20.0   \n",
       "3        4.582491       266.0           31.0   \n",
       "4        4.924731       365.0            7.0   \n",
       "\n",
       "                                           subtitles  sentiment  \n",
       "0     presidents of the United States mr. go   tr...          1  \n",
       "1     it's now officially Tuesday November a   di...          1  \n",
       "2     you   [Music]   [Music]   [Music]   you   I...          1  \n",
       "3     we welcome stars and president   [Music]   ...          1  \n",
       "4     you   thank you   [Music]   great people Gr...          1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pruneRating(x):\n",
    "    '''\n",
    "    utility function for lambda\n",
    "    returns 1 for good/avg rating\n",
    "    returns 0 for bad rating\n",
    "    '''\n",
    "    if x >= avg_score:\n",
    "        return 1\n",
    "    if x < avg_score:\n",
    "        return 0\n",
    "\n",
    "df['sentiment'] = df['average_rating'].apply(\n",
    "    lambda x: pruneRating(x)\n",
    ")\n",
    "\n",
    "enc3 = LabelEncoder()\n",
    "enc3.fit(df['sentiment'])\n",
    "\n",
    "print(enc3.classes_)\n",
    "print(sep)\n",
    "print(df['sentiment'].describe()) \n",
    "print(sep)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "- Clean the subtitles data and store the cleaned text in a new column 'subtitle_clean'.\n",
    "    * For each step of your text cleaning give a brief explanation of why you chose to perform that method on the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block of code will take a little while to process since the text is being thoroughly cleaned.\n",
    "wnl = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation.\n",
    "    2. Remove all stopwords.\n",
    "    4. Convert all remaining words to lowercase.\n",
    "        - if the string has numbers in it, discard it.\n",
    "    5. Lemmatize.\n",
    "    6. Convert to the stem of the word.\n",
    "    7. Returns a list of the cleaned text.\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    # convert all words to lowercase and lemmatize and stem\n",
    "    \n",
    "    arr = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = porter.stem(word)\n",
    "        word = wnl.lemmatize(word)\n",
    "        if word.isalpha():\n",
    "            word += ' '\n",
    "            arr.append(word)\n",
    "        \n",
    "    arr = ''.join(arr) # convert the array of words/tokens into a string\n",
    "#     print(arr,sep)\n",
    "    if len(arr) > 0:\n",
    "        return arr\n",
    "    else:\n",
    "        return \"smeckledorf\" # if the entire subtitle had no words in it.\n",
    "\n",
    "def test_text_process():\n",
    "    '''\n",
    "    function to hold some test cases to ensure the lambda function does its job\n",
    "    '''\n",
    "    print(\n",
    "        'Test stopwords and lowercase:\\nHI I AM A STOPWORD = ', \n",
    "        text_process(\"HI I AM A STOPWORD\")\n",
    "    )\n",
    "    print(sep)\n",
    "    print(\n",
    "        'Test Stemming words and removing punctuation:\\nfishing, fisher, fished = ', \n",
    "        text_process(\"fishing, fisher , fished\")\n",
    "    )\n",
    "\n",
    "def pruneSubtitle(x):\n",
    "    '''\n",
    "    utility function for lambda\n",
    "    x - string from `subtitles` col\n",
    "    '''\n",
    "    return text_process(x)\n",
    "\n",
    "# test_text_process()\n",
    "\n",
    "df['subtitle_clean'] = df['subtitles'].apply(\n",
    "    lambda x: pruneSubtitle(x)\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to Lemmatize AND Stem each word since the amount of text being processed is very large, and this will also allow the models to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Use TFIDFVectorizer and CountVectorizer to encode the cleaned subtitles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # define the data to use\n",
    "clean_subs = list(df['subtitle_clean']) \n",
    "# clean_subs = list(df['subtitle_clean']) # this line might be crashing the file \n",
    "# # clean_subs = df['subtitle_clean'].values # kinda pointless, parsing as list() is neater\n",
    "# # jupyter might require more memory to run this\n",
    "\n",
    "print(type(clean_subs))\n",
    "print(sep)\n",
    "print(len(clean_subs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf = tf_vectorizer.fit_transform(clean_subs)\n",
    "print(\"CountVectorizer:\")\n",
    "print(tf)\n",
    "print(sep)\n",
    "print(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF \n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf = tfidf_vec.fit_transform(clean_subs)\n",
    "print(\"TFIDF:\")\n",
    "print(tfidf)\n",
    "print(sep)\n",
    "print(tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification (30 marks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "When choosing a metric to access the performance of your classifier provide a brief explanation of why you chose that metric.\n",
    "\n",
    "Overall, both methods are good supervised machine learning classification methods.\n",
    "\n",
    "- Logistic Regression:\n",
    "    * very good option since the target varaible (sentiment column) is binary (0,1).\n",
    "    * the encoded data from TF and TFIDF supply good values.\n",
    "\n",
    "- SVM:\n",
    "    * also good at solving regression problems like this.\n",
    "    * similarly to logistic regression as a model: it also excels at solving a non-probabilistic binary problem.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Perform the following classification experiments keeping track of the performance of each classification task for future use: \n",
    "1. Logistic regression model on word count \n",
    "2. Logistic regression model on TFIDF \n",
    "3. Logistic regression model on TFIDF + ngram \n",
    "4. Support Vector Machine model on word count \n",
    "5. Support Vector Machine model on TFIDF \n",
    "6. Support Vector Machine model on TFIDF + ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_fit(X, y, model, clf_model):\n",
    "    \"\"\"\n",
    "    X - data\n",
    "    y - target variables\n",
    "    model - model for the word processing, tfidf, countvectorization etc\n",
    "    clf_model - classification algorithm, classifier (in this case, logistic regression)\n",
    "    coef_show - show the top coeefs if true (1)\n",
    "    \"\"\"\n",
    "    X_c = model.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_c, y, random_state=0)\n",
    "    clf = clf_model.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "#     print('type(X_c): ', type(X_c)) # sparse matrix\n",
    "    print('# features: {}'.format(X_c.shape[1]))\n",
    "    print('# train records: {}'.format(X_train.shape[0]))\n",
    "    print('# test records: {}'.format(X_test.shape[0]))\n",
    "    print('Model Recall: {}'.format(recall))\n",
    "    print('Model Accuracy: {}'.format(acc))\n",
    "    \n",
    "    w = model.get_feature_names()\n",
    "    coef = clf.coef_.tolist()[0] # ??\n",
    "    coeff_df = pd.DataFrame({\n",
    "        'Word': w, 'Coefficient': coef\n",
    "    })\n",
    "    coeff_df = coeff_df.sort_values(\n",
    "        ['Coefficient', 'Word'], ascending=[0,1]\n",
    "    )\n",
    "    print(sep)\n",
    "    print('-Top 5 positive-')\n",
    "    print(coeff_df.head(5).to_string(index=False))\n",
    "    print(sep)\n",
    "    print('-Top 5 negative')\n",
    "    print(coeff_df.tail(5).to_string(index=False))\n",
    "    \n",
    "    return recall, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['average_rating'] != avg_score] # remove mean values so the classifiers give more meaningful predictions\n",
    "X = df['subtitle_clean'] # data\n",
    "y = df['sentiment'] # target variable\n",
    "\n",
    "c = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression and SVM models on word count (tf)\n",
    "recall_LR_tf, acc_LR_tf = text_fit(X, y, c, LogisticRegression())\n",
    "print(sep)\n",
    "recall_SVM_tf, acc_SVM_tf = text_fit(X, y, c, LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression and SVM on tfidf\n",
    "recall_LR_tfidf, acc_LR_tfidf = text_fit(X, y, tfidf, LogisticRegression())\n",
    "print(sep)\n",
    "recall_SVM_tfidf, acc_SVM_tfidf = text_fit(X, y, tfidf, LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression and SVM on tfidf and ngram\n",
    "recall_LR_tfidf_ngram, acc_LR_tfidf_ngram = text_fit(X, y, TfidfVectorizer(ngram_range=(1,2)), LogisticRegression())\n",
    "print(sep)\n",
    "recall_SVM_tfidf_ngram, acc_SVM_tfidf_ngram = text_fit(X, y, TfidfVectorizer(ngram_range=(1,2)), LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'LR on TF', 'SVM on TF',\n",
    "        'LR on TFIDF', 'SVM on TFIDF',\n",
    "        'LR on TFIDF w/ Ngram=2', 'SVM on TFIDF w/ Ngram=2'\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_LR_tf, recall_SVM_tf,\n",
    "        recall_LR_tfidf, recall_SVM_tfidf,\n",
    "        recall_LR_tfidf_ngram, recall_SVM_tfidf_ngram\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        acc_LR_tf, acc_SVM_tf,\n",
    "        acc_LR_tfidf, acc_SVM_tfidf,\n",
    "        acc_LR_tfidf_ngram, acc_SVM_tfidf_ngram\n",
    "    ],\n",
    "})\n",
    "\n",
    "models.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Recall', y='Model', data=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Accuracy', y='Model', data=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both models are performing very well on the data while using both text classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Topic Modeling\n",
    "Using TFIDF and Count Vectorizer models imported for sklearn, perform topic modelling using the following topic modeling algorithms:\n",
    "1. NMF\n",
    "2. LDA\n",
    "3. SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up function for displaying the topics per document\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    '''\n",
    "    H, W -> feature matricies\n",
    "    feature_names -> labels\n",
    "    documents -> text/data\n",
    "    no_top_words -> number of topic words\n",
    "    no_top_documents -> number of topic documents\n",
    "    '''\n",
    "    top_docs = []\n",
    "    for topic_idx, topic in enumerate(H): # iterate through H\n",
    "        print(\"Topic %d: \" % (topic_idx)) # speech at index i\n",
    "        print(\" \".join([\n",
    "            feature_names[i]\n",
    "            for i in topic.argsort()[:-no_top_words - 1:-1]\n",
    "        ]))\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        top_doc_indicies = np.argsort(W[:,topic_idx])[::-1][0:no_top_documents]\n",
    "        \n",
    "        doc_str = \"\"\n",
    "        for doc_index in top_doc_indicies:\n",
    "            doc_str += documents[doc_index]\n",
    "            print(documents[doc_index])\n",
    "            print(\"\\n\")\n",
    "            print(sep)\n",
    "        top_docs.append(doc_str)\n",
    "        \n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the data for topic modeling\n",
    "documents = list(X)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf model for NMF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw term counts for LDA and SVD\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "When choosing the number of topics give a brief explanation of why that number was chosen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to display 5 topics per model since the amount of text per row is so large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 15\n",
    "no_top_words = 10\n",
    "no_top_documents = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF\n",
    "nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_W = nmf_model.transform(tfidf)\n",
    "nmf_H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD\n",
    "lsi_model = TruncatedSVD(n_components=no_topics, n_iter=7, random_state=42).fit(tf)\n",
    "lsi_W = lsi_model.transform(tf)\n",
    "lsi_H = lsi_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"NMF Topics \\n\\n\")\n",
    "nmf_top_docs = display_topics(nmf_H, nmf_W, tfidf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\nLDA Topics \\n\\n\")\n",
    "lda_top_docs = display_topics(lda_H, lda_W, tf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\nSVD Topics \\n\\n\")\n",
    "lsi_top_docs = display_topics(lsi_H, lsi_W, tf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Discuss based on the top 10 words each of the algorithms choose, for each topic cluster, what category the topics fall under. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< write answer here i guess XD>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the clusters obtained from a topic model algorithm from above and plot a word cloud for each of the clusters. For example, if the number of topics chosen was 10 and the topics were obtained from the SVD algorithm, 10 word clouds should be plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size']=12                 \n",
    "plt.rcParams['savefig.dpi']=1200             \n",
    "plt.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "def print_cloud(data):\n",
    "    '''\n",
    "    plot a wordcloud with the supplied data.\n",
    "    \n",
    "    data -> document/sentence/string\n",
    "    '''\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        random_state=42\n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "\n",
    "def print_cloud_from_docs(docs_list):\n",
    "    '''\n",
    "    prints wordclouds from a list of documents.\n",
    "    \n",
    "    doc_list -> list of documents\n",
    "    '''\n",
    "    for doc in docs_list:\n",
    "        print_cloud(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print word clouds for each classifier\n",
    "print(\"Word clouds from NMF:\")\n",
    "print(len(nmf_top_docs))\n",
    "print_cloud_from_docs(nmf_top_docs)\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word clouds from LDA:\")\n",
    "print_cloud_from_docs(lda_top_docs)\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word clouds from SVD:\")\n",
    "print_cloud_from_docs(lsi_top_docs)\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
